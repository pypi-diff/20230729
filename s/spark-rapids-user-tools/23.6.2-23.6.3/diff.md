# Comparing `tmp/spark_rapids_user_tools-23.6.2-156_f44974b-py3-none-any.whl.zip` & `tmp/spark_rapids_user_tools-23.6.3-162_7d96f24-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,70 +1,71 @@
-Zip file size: 159564 bytes, number of entries: 68
--rw-r--r--  2.0 unx      750 b- defN 23-Jul-14 02:43 spark_rapids_pytools/__init__.py
--rw-r--r--  2.0 unx      992 b- defN 23-Jul-14 02:43 spark_rapids_pytools/build.py
--rw-r--r--  2.0 unx     1324 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrapper.py
--rw-r--r--  2.0 unx      627 b- defN 23-Jul-14 02:43 spark_rapids_pytools/ascli_cli/__init__.py
--rw-r--r--  2.0 unx     8270 b- defN 23-Jul-14 02:43 spark_rapids_pytools/ascli_cli/ascli.py
--rw-r--r--  2.0 unx      646 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/__init__.py
--rw-r--r--  2.0 unx     8507 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/azurestorage.py
--rw-r--r--  2.0 unx    12662 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/databricks_aws.py
--rw-r--r--  2.0 unx     1268 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/databricks_aws_job.py
--rw-r--r--  2.0 unx    16308 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/databricks_azure.py
--rw-r--r--  2.0 unx     2428 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/databricks_azure_job.py
--rw-r--r--  2.0 unx    24674 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/dataproc.py
--rw-r--r--  2.0 unx      922 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/dataproc_job.py
--rw-r--r--  2.0 unx    21033 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/emr.py
--rw-r--r--  2.0 unx     1252 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/emr_job.py
--rw-r--r--  2.0 unx     4939 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/gstorage.py
--rw-r--r--  2.0 unx    13387 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/onprem.py
--rw-r--r--  2.0 unx     4055 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/s3storage.py
--rw-r--r--  2.0 unx    51024 b- defN 23-Jul-14 02:43 spark_rapids_pytools/cloud_api/sp_types.py
--rw-r--r--  2.0 unx      658 b- defN 23-Jul-14 02:43 spark_rapids_pytools/common/__init__.py
--rw-r--r--  2.0 unx      978 b- defN 23-Jul-14 02:43 spark_rapids_pytools/common/exceptions.py
--rw-r--r--  2.0 unx     5432 b- defN 23-Jul-14 02:43 spark_rapids_pytools/common/prop_manager.py
--rw-r--r--  2.0 unx    14712 b- defN 23-Jul-14 02:43 spark_rapids_pytools/common/sys_storage.py
--rw-r--r--  2.0 unx    14006 b- defN 23-Jul-14 02:43 spark_rapids_pytools/common/utilities.py
--rw-r--r--  2.0 unx      659 b- defN 23-Jul-14 02:43 spark_rapids_pytools/pricing/__init__.py
--rw-r--r--  2.0 unx     3429 b- defN 23-Jul-14 02:43 spark_rapids_pytools/pricing/databricks_azure_pricing.py
--rw-r--r--  2.0 unx     3522 b- defN 23-Jul-14 02:43 spark_rapids_pytools/pricing/databricks_pricing.py
--rw-r--r--  2.0 unx     4247 b- defN 23-Jul-14 02:43 spark_rapids_pytools/pricing/dataproc_pricing.py
--rw-r--r--  2.0 unx     4717 b- defN 23-Jul-14 02:43 spark_rapids_pytools/pricing/emr_pricing.py
--rw-r--r--  2.0 unx     6400 b- defN 23-Jul-14 02:43 spark_rapids_pytools/pricing/price_provider.py
--rw-r--r--  2.0 unx      666 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/__init__.py
--rw-r--r--  2.0 unx     8830 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/bootstrap.py
--rw-r--r--  2.0 unx     6546 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/diagnostic.py
--rw-r--r--  2.0 unx    14549 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/profiling.py
--rw-r--r--  2.0 unx    38670 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/qualification.py
--rw-r--r--  2.0 unx     5739 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/rapids_job.py
--rw-r--r--  2.0 unx    33598 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/rapids_tool.py
--rw-r--r--  2.0 unx     6576 b- defN 23-Jul-14 02:43 spark_rapids_pytools/rapids/tool_ctxt.py
--rw-r--r--  2.0 unx     1390 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/bootstrap-conf.yaml
--rwxr-xr-x  2.0 unx     4453 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/collect.sh
--rw-r--r--  2.0 unx    30566 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/databricks-premium-catalog.json
--rw-r--r--  2.0 unx    10448 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/databricks_aws-configs.json
--rw-r--r--  2.0 unx    10017 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/databricks_azure-configs.json
--rw-r--r--  2.0 unx     8047 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/dataproc-configs.json
--rw-r--r--  2.0 unx       30 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/diagnostic-conf.yaml
--rw-r--r--  2.0 unx     8994 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/emr-configs.json
--rw-r--r--  2.0 unx     4727 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/onprem-configs.json
--rw-r--r--  2.0 unx    38903 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/premium-databricks-azure-catalog.json
--rw-r--r--  2.0 unx     1524 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/profiling-conf.yaml
--rw-r--r--  2.0 unx     4273 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/qualification-conf.yaml
--rw-r--r--  2.0 unx     2195 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/dev/process_databricks_azure_pricing.py
--rw-r--r--  2.0 unx      671 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
--rw-r--r--  2.0 unx      518 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
--rw-r--r--  2.0 unx      855 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
--rw-r--r--  2.0 unx      537 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
--rw-r--r--  2.0 unx      424 b- defN 23-Jul-14 02:43 spark_rapids_pytools/resources/templates/info_recommendations_disabled.ms
--rw-r--r--  2.0 unx      630 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrappers/__init__.py
--rw-r--r--  2.0 unx    13207 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
--rw-r--r--  2.0 unx    12960 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrappers/databricks_azure_wrapper.py
--rw-r--r--  2.0 unx    15910 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrappers/dataproc_wrapper.py
--rw-r--r--  2.0 unx    12086 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrappers/emr_wrapper.py
--rw-r--r--  2.0 unx     9829 b- defN 23-Jul-14 02:43 spark_rapids_pytools/wrappers/onprem_wrapper.py
--rw-r--r--  2.0 unx    21086 b- defN 23-Jul-14 02:43 spark_rapids_user_tools-23.6.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     2927 b- defN 23-Jul-14 02:43 spark_rapids_user_tools-23.6.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-14 02:43 spark_rapids_user_tools-23.6.2.dist-info/WHEEL
--rw-r--r--  2.0 unx      128 b- defN 23-Jul-14 02:43 spark_rapids_user_tools-23.6.2.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       21 b- defN 23-Jul-14 02:43 spark_rapids_user_tools-23.6.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7091 b- defN 23-Jul-14 02:43 spark_rapids_user_tools-23.6.2.dist-info/RECORD
-68 files, 573541 bytes uncompressed, 147832 bytes compressed:  74.2%
+Zip file size: 161197 bytes, number of entries: 69
+-rw-r--r--  2.0 unx      750 b- defN 23-Jul-28 21:02 spark_rapids_pytools/__init__.py
+-rw-r--r--  2.0 unx      992 b- defN 23-Jul-28 21:02 spark_rapids_pytools/build.py
+-rw-r--r--  2.0 unx     1324 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrapper.py
+-rw-r--r--  2.0 unx      627 b- defN 23-Jul-28 21:02 spark_rapids_pytools/ascli_cli/__init__.py
+-rw-r--r--  2.0 unx     8548 b- defN 23-Jul-28 21:02 spark_rapids_pytools/ascli_cli/ascli.py
+-rw-r--r--  2.0 unx      646 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/__init__.py
+-rw-r--r--  2.0 unx     8507 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/azurestorage.py
+-rw-r--r--  2.0 unx    12662 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/databricks_aws.py
+-rw-r--r--  2.0 unx     1268 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/databricks_aws_job.py
+-rw-r--r--  2.0 unx    16308 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/databricks_azure.py
+-rw-r--r--  2.0 unx     2428 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/databricks_azure_job.py
+-rw-r--r--  2.0 unx    24674 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/dataproc.py
+-rw-r--r--  2.0 unx      922 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/dataproc_job.py
+-rw-r--r--  2.0 unx    21033 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/emr.py
+-rw-r--r--  2.0 unx     1252 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/emr_job.py
+-rw-r--r--  2.0 unx     4939 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/gstorage.py
+-rw-r--r--  2.0 unx    13310 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/onprem.py
+-rw-r--r--  2.0 unx     4055 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/s3storage.py
+-rw-r--r--  2.0 unx    51086 b- defN 23-Jul-28 21:02 spark_rapids_pytools/cloud_api/sp_types.py
+-rw-r--r--  2.0 unx      658 b- defN 23-Jul-28 21:02 spark_rapids_pytools/common/__init__.py
+-rw-r--r--  2.0 unx      978 b- defN 23-Jul-28 21:02 spark_rapids_pytools/common/exceptions.py
+-rw-r--r--  2.0 unx     5432 b- defN 23-Jul-28 21:02 spark_rapids_pytools/common/prop_manager.py
+-rw-r--r--  2.0 unx    14712 b- defN 23-Jul-28 21:02 spark_rapids_pytools/common/sys_storage.py
+-rw-r--r--  2.0 unx    14006 b- defN 23-Jul-28 21:02 spark_rapids_pytools/common/utilities.py
+-rw-r--r--  2.0 unx      659 b- defN 23-Jul-28 21:02 spark_rapids_pytools/pricing/__init__.py
+-rw-r--r--  2.0 unx     3429 b- defN 23-Jul-28 21:02 spark_rapids_pytools/pricing/databricks_azure_pricing.py
+-rw-r--r--  2.0 unx     3522 b- defN 23-Jul-28 21:02 spark_rapids_pytools/pricing/databricks_pricing.py
+-rw-r--r--  2.0 unx     4247 b- defN 23-Jul-28 21:02 spark_rapids_pytools/pricing/dataproc_pricing.py
+-rw-r--r--  2.0 unx     4717 b- defN 23-Jul-28 21:02 spark_rapids_pytools/pricing/emr_pricing.py
+-rw-r--r--  2.0 unx     6400 b- defN 23-Jul-28 21:02 spark_rapids_pytools/pricing/price_provider.py
+-rw-r--r--  2.0 unx      666 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/__init__.py
+-rw-r--r--  2.0 unx     6026 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/bootstrap.py
+-rw-r--r--  2.0 unx     6546 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/diagnostic.py
+-rw-r--r--  2.0 unx    14549 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/profiling.py
+-rw-r--r--  2.0 unx    42806 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/qualification.py
+-rw-r--r--  2.0 unx     5739 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/rapids_job.py
+-rw-r--r--  2.0 unx    36808 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/rapids_tool.py
+-rw-r--r--  2.0 unx     6576 b- defN 23-Jul-28 21:02 spark_rapids_pytools/rapids/tool_ctxt.py
+-rw-r--r--  2.0 unx      273 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/bootstrap-conf.yaml
+-rw-r--r--  2.0 unx     1073 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/cluster-configs.yaml
+-rwxr-xr-x  2.0 unx     4453 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/collect.sh
+-rw-r--r--  2.0 unx    30566 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/databricks-premium-catalog.json
+-rw-r--r--  2.0 unx    10448 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/databricks_aws-configs.json
+-rw-r--r--  2.0 unx    10017 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/databricks_azure-configs.json
+-rw-r--r--  2.0 unx     8872 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/dataproc-configs.json
+-rw-r--r--  2.0 unx       30 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/diagnostic-conf.yaml
+-rw-r--r--  2.0 unx     9760 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/emr-configs.json
+-rw-r--r--  2.0 unx     4727 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/onprem-configs.json
+-rw-r--r--  2.0 unx    38903 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/premium-databricks-azure-catalog.json
+-rw-r--r--  2.0 unx     1524 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/profiling-conf.yaml
+-rw-r--r--  2.0 unx     4762 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/qualification-conf.yaml
+-rw-r--r--  2.0 unx     2195 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/dev/process_databricks_azure_pricing.py
+-rw-r--r--  2.0 unx      671 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      518 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms
+-rw-r--r--  2.0 unx      855 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms
+-rw-r--r--  2.0 unx      537 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms
+-rw-r--r--  2.0 unx      424 b- defN 23-Jul-28 21:02 spark_rapids_pytools/resources/templates/info_recommendations_disabled.ms
+-rw-r--r--  2.0 unx      630 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrappers/__init__.py
+-rw-r--r--  2.0 unx    13214 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrappers/databricks_aws_wrapper.py
+-rw-r--r--  2.0 unx    12967 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrappers/databricks_azure_wrapper.py
+-rw-r--r--  2.0 unx    15917 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrappers/dataproc_wrapper.py
+-rw-r--r--  2.0 unx    12093 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrappers/emr_wrapper.py
+-rw-r--r--  2.0 unx     9828 b- defN 23-Jul-28 21:02 spark_rapids_pytools/wrappers/onprem_wrapper.py
+-rw-r--r--  2.0 unx    21086 b- defN 23-Jul-28 21:03 spark_rapids_user_tools-23.6.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2927 b- defN 23-Jul-28 21:03 spark_rapids_user_tools-23.6.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-28 21:03 spark_rapids_user_tools-23.6.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx      128 b- defN 23-Jul-28 21:03 spark_rapids_user_tools-23.6.3.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       21 b- defN 23-Jul-28 21:03 spark_rapids_user_tools-23.6.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7198 b- defN 23-Jul-28 21:03 spark_rapids_user_tools-23.6.3.dist-info/RECORD
+69 files, 580516 bytes uncompressed, 149287 bytes compressed:  74.3%
```

## zipnote {}

```diff
@@ -111,14 +111,17 @@
 
 Filename: spark_rapids_pytools/rapids/tool_ctxt.py
 Comment: 
 
 Filename: spark_rapids_pytools/resources/bootstrap-conf.yaml
 Comment: 
 
+Filename: spark_rapids_pytools/resources/cluster-configs.yaml
+Comment: 
+
 Filename: spark_rapids_pytools/resources/collect.sh
 Comment: 
 
 Filename: spark_rapids_pytools/resources/databricks-premium-catalog.json
 Comment: 
 
 Filename: spark_rapids_pytools/resources/databricks_aws-configs.json
@@ -180,26 +183,26 @@
 
 Filename: spark_rapids_pytools/wrappers/emr_wrapper.py
 Comment: 
 
 Filename: spark_rapids_pytools/wrappers/onprem_wrapper.py
 Comment: 
 
-Filename: spark_rapids_user_tools-23.6.2.dist-info/LICENSE
+Filename: spark_rapids_user_tools-23.6.3.dist-info/LICENSE
 Comment: 
 
-Filename: spark_rapids_user_tools-23.6.2.dist-info/METADATA
+Filename: spark_rapids_user_tools-23.6.3.dist-info/METADATA
 Comment: 
 
-Filename: spark_rapids_user_tools-23.6.2.dist-info/WHEEL
+Filename: spark_rapids_user_tools-23.6.3.dist-info/WHEEL
 Comment: 
 
-Filename: spark_rapids_user_tools-23.6.2.dist-info/entry_points.txt
+Filename: spark_rapids_user_tools-23.6.3.dist-info/entry_points.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-23.6.2.dist-info/top_level.txt
+Filename: spark_rapids_user_tools-23.6.3.dist-info/top_level.txt
 Comment: 
 
-Filename: spark_rapids_user_tools-23.6.2.dist-info/RECORD
+Filename: spark_rapids_user_tools-23.6.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spark_rapids_pytools/__init__.py

```diff
@@ -12,9 +12,9 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """init file of the spark_rapids_pytools package."""
 
 from spark_rapids_pytools.build import get_version
 
-VERSION = '23.06.2'
+VERSION = '23.06.3'
 __version__ = get_version(VERSION)
```

## spark_rapids_pytools/ascli_cli/ascli.py

```diff
@@ -138,31 +138,37 @@
                 It can either be a CSP-cluster name or a path to the cluster/worker's info properties
                 file (json format).
         :param platform: defines one of the following "onprem", "emr", "dataproc", "databricks-aws",
                 and "databricks-azure".
         :param output_folder: path to store the output
         """
 
+        runtime_platform = CloudPlatform.fromstring(platform)
+        auto_tuner_file_input = None
+        gpu_cluster = cluster
+        if runtime_platform == CloudPlatform.ONPREM:
+            auto_tuner_file_input = cluster
+            gpu_cluster = None
         wrapper_prof_options = {
             'platformOpts': {
                 'credentialFile': None,
                 'deployMode': DeployMode.LOCAL,
             },
             'migrationClustersProps': {
-                'gpuCluster': cluster
+                'gpuCluster': gpu_cluster
             },
             'jobSubmissionProps': {
                 'remoteFolder': None,
                 'platformArgs': {
                     'jvmMaxHeapSize': 24
                 }
             },
             'eventlogs': eventlogs,
             'toolsJar': None,
-            'autoTunerFileInput': None
+            'autoTunerFileInput': auto_tuner_file_input
         }
         prof_tool_obj = ProfilingAsLocal(platform_type=CloudPlatform.fromstring(platform),
                                          output_folder=output_folder,
                                          wrapper_options=wrapper_prof_options)
         return prof_tool_obj.launch()
```

## spark_rapids_pytools/cloud_api/onprem.py

```diff
@@ -56,19 +56,17 @@
 
     def migrate_cluster_to_gpu(self, orig_cluster):
         """
             given a cluster, convert it to run NVIDIA Gpu based on mapping instance types
             :param orig_cluster: the original cluster to migrate from
             :return: a new object cluster that supports GPU.
         """
-        if orig_cluster is not None:
-            gpu_cluster_ob = OnPremCluster(self)
-            gpu_cluster_ob.migrate_from_cluster(orig_cluster)
-            return gpu_cluster_ob
-        return orig_cluster
+        gpu_cluster_ob = OnPremCluster(self)
+        gpu_cluster_ob.migrate_from_cluster(orig_cluster)
+        return gpu_cluster_ob
 
     def get_platform_name(self) -> str:
         """
         This used to get the lower case of the platform of the runtime.
         :return: the name of the platform of the runtime in lower_case.
         """
         if self.platform is not None:
```

## spark_rapids_pytools/cloud_api/sp_types.py

```diff
@@ -78,28 +78,30 @@
     T4 = 't4'
     V100 = 'v100'
     K80 = 'k80'
     A100 = 'a100'
     P100 = 'P100'
     P4 = 'P4'
     L4 = 'l4'
+    A10 = 'a10'
 
     @classmethod
     def get_default_gpu(cls):
         return cls.T4
 
     def get_gpu_mem(self) -> list:
         memory_hash = {
             self.T4: [16384],
             self.L4: [24576],
             self.A100: [40960, 81920],
             self.P4: [8192],
             self.K80: [12288],
             self.V100: [16384],
-            self.P100: [16384]
+            self.P100: [16384],
+            self.A10: [24576]
         }
         return memory_hash.get(self)
 
 
 class ClusterState(EnumeratedType):
     """
     Standard states for a cluster.
@@ -634,15 +636,15 @@
     storage: StorageDriver = field(default=None, init=False)
     ctxt: dict = field(default_factory=dict, init=False)
     configs: JSONPropertiesContainer = field(default=None, init=False)
     logger: Logger = field(default=ToolLogging.get_and_setup_logger('rapids.tools.csp'), init=False)
 
     @classmethod
     def list_supported_gpus(cls):
-        return [GpuDevice.T4, GpuDevice.A100, GpuDevice.L4]
+        return [GpuDevice.T4, GpuDevice.A100, GpuDevice.L4, GpuDevice.A10]
 
     def load_from_config_parser(self, conf_file, **prop_args) -> dict:
         res = None
         try:
             parser_obj = configparser.ConfigParser()
             parser_obj.read(conf_file)
             res = {}
```

## spark_rapids_pytools/rapids/bootstrap.py

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Implementation class representing wrapper around the RAPIDS acceleration Bootstrap tool."""
 
 from dataclasses import dataclass
 
-from spark_rapids_pytools.cloud_api.sp_types import ClusterBase, NodeHWInfo
+from spark_rapids_pytools.cloud_api.sp_types import ClusterBase
 from spark_rapids_pytools.common.sys_storage import FSUtil
 from spark_rapids_pytools.common.utilities import Utils
 from spark_rapids_pytools.rapids.rapids_tool import RapidsTool
 
 
 @dataclass
 class Bootstrap(RapidsTool):
@@ -32,70 +32,25 @@
     def _process_custom_args(self):
         dry_run_opt = self.wrapper_options.get('dryRun', 'False')
         self.ctxt.set_ctxt('dryRunOpt', bool(dry_run_opt))
 
     def requires_cluster_connection(self) -> bool:
         return True
 
-    def __calculate_spark_settings(self, worker_info: NodeHWInfo) -> dict:
-        """
-        Calculate the cluster properties that we need to append to the /etc/defaults of the spark
-        if necessary.
-        :param worker_info: the hardware info as extracted from the worker. Note that we assume
-                            that all the workers have the same configurations.
-        :return: dictionary containing 7 spark properties to be set by default on the cluster.
-        """
-        num_gpus = worker_info.gpu_info.num_gpus
-        gpu_mem = worker_info.gpu_info.gpu_mem
-        num_cpus = worker_info.sys_info.num_cpus
-        cpu_mem = worker_info.sys_info.cpu_mem
-
-        constants = self.ctxt.get_value('local', 'clusterConfigs', 'constants')
-        executors_per_node = num_gpus
-        num_executor_cores = max(1, num_cpus // executors_per_node)
-        gpu_concurrent_tasks = min(constants.get('maxGpuConcurrent'), gpu_mem // constants.get('gpuMemPerTaskMB'))
-        # account for system overhead
-        usable_worker_mem = max(0, cpu_mem - constants.get('systemReserveMB'))
-        executor_container_mem = usable_worker_mem // executors_per_node
-        # reserve 10% of heap as memory overhead
-        max_executor_heap = max(0, int(executor_container_mem * (1 - constants.get('heapOverheadFraction'))))
-        # give up to 2GB of heap to each executor core
-        executor_heap = min(max_executor_heap, constants.get('heapPerCoreMB') * num_executor_cores)
-        executor_mem_overhead = int(executor_heap * constants.get('heapOverheadFraction'))
-        # use default for pageable_pool to add to memory overhead
-        pageable_pool = constants.get('defaultPageablePoolMB')
-        # pinned memory uses any unused space up to 4GB
-        pinned_mem = min(constants.get('maxPinnedMemoryMB'),
-                         executor_container_mem - executor_heap - executor_mem_overhead - pageable_pool)
-        executor_mem_overhead += pinned_mem + pageable_pool
-        res = {
-            'spark.executor.cores': num_executor_cores,
-            'spark.executor.memory': f'{executor_heap}m',
-            'spark.executor.memoryOverhead': f'{executor_mem_overhead}m',
-            'spark.rapids.sql.concurrentGpuTasks': gpu_concurrent_tasks,
-            'spark.rapids.memory.pinnedPool.size': f'{pinned_mem}m',
-            'spark.sql.files.maxPartitionBytes': f'{constants.get("maxSqlFilesPartitionsMB")}m',
-            'spark.task.resource.gpu.amount': 1 / num_executor_cores,
-            'spark.rapids.shuffle.multiThreaded.reader.threads': num_executor_cores,
-            'spark.rapids.shuffle.multiThreaded.writer.threads': num_executor_cores,
-            'spark.rapids.sql.multiThreadedRead.numThreads': max(20, num_executor_cores)
-        }
-        return res
-
     def _run_rapids_tool(self):
         """
         Run the bootstrap on the driver node
         :return:
         """
         self.logger.info('Executing Bootstrap commands on remote cluster to calculate default configurations.')
         exec_cluster: ClusterBase = self.get_exec_cluster()
         worker_hw_info = exec_cluster.get_worker_hw_info()
         self.logger.debug('Worker hardware INFO %s', worker_hw_info)
         try:
-            spark_settings = self.__calculate_spark_settings(worker_info=worker_hw_info)
+            spark_settings = self._calculate_spark_settings(worker_info=worker_hw_info)
             self.ctxt.set_ctxt('bootstrap_results', spark_settings)
             self.logger.debug('%s Tool finished calculating recommended Apache Spark configurations for cluster %s: %s',
                               self.pretty_name(),
                               self.cluster,
                               str(spark_settings))
         except Exception as e:
             self.logger.error('Error while calculating spark configurations')
```

## spark_rapids_pytools/rapids/qualification.py

```diff
@@ -18,15 +18,15 @@
 from dataclasses import dataclass, field
 from math import ceil
 from typing import Any, List, Callable
 
 import pandas as pd
 from tabulate import tabulate
 
-from spark_rapids_pytools.cloud_api.sp_types import EnumeratedType, ClusterReshape
+from spark_rapids_pytools.cloud_api.sp_types import EnumeratedType, ClusterReshape, NodeHWInfo
 from spark_rapids_pytools.common.sys_storage import FSUtil
 from spark_rapids_pytools.common.utilities import Utils, TemplateGenerator
 from spark_rapids_pytools.pricing.price_provider import SavingsEstimator
 from spark_rapids_pytools.rapids.rapids_tool import RapidsJarTool
 
 
 class QualFilterApp(EnumeratedType):
@@ -53,15 +53,15 @@
     Encapsulates the logic to organize Qualification report.
     """
     comments: Any = None
     all_apps: pd.DataFrame = None
     recommended_apps: pd.DataFrame = None
     df_result: pd.DataFrame = None
     irrelevant_speedups: bool = False
-    pricing_config: Any = None
+    savings_report_flag: bool = False
     sections_generators: List[Callable] = field(default_factory=lambda: [])
 
     def _get_total_durations(self) -> int:
         if not self.is_empty():
             return self.all_apps['App Duration'].sum()
         return 0
 
@@ -136,26 +136,26 @@
             if pretty_df.empty:
                 # the results were reduced to no rows because of the filters
                 report_content.append(
                     f'{app_name} tool found no qualified applications after applying the filters.\n'
                     f'See the CSV file for full report or disable the filters.')
             else:
                 report_content.append(tabulate(pretty_df, headers='keys', tablefmt='psql', floatfmt='.2f'))
-        elif self.pricing_config is None:
+        elif not self.savings_report_flag:
             report_content.append(f'pricing information not found for ${app_name}')
         else:
             report_content.append(f'{app_name} tool found no records to show.')
 
         overall_speedup = 0.0
         total_apps_durations = 1.0 * self._get_total_durations()
         total_gpu_durations = self._get_total_gpu_durations()
         if total_gpu_durations > 0:
             overall_speedup = total_apps_durations / total_gpu_durations
 
-        if self.pricing_config is None:
+        if not self.savings_report_flag:
             report_content.append(Utils.gen_report_sec_header('Report Summary', hrule=False))
             report_summary = [['Total applications', self._get_stats_total_apps()],
                               ['Overall estimated speedup', format_float(overall_speedup)]]
         else:
             total_app_cost = self._get_stats_total_cost()
             total_gpu_cost = self._get_stats_total_gpu_cost()
             estimated_gpu_savings = 0.0
@@ -171,15 +171,16 @@
             report_summary.insert(1, ['RAPIDS candidates', self._get_stats_recommended_apps()])
         report_content.append(tabulate(report_summary, colalign=('left', 'right')))
         if self.comments:
             report_content.append(Utils.gen_report_sec_header('Notes'))
             report_content.extend(f' - {line}' for line in self.comments)
         if self.sections_generators:
             for section_generator in self.sections_generators:
-                report_content.append(Utils.gen_multiline_str(section_generator()))
+                if section_generator:
+                    report_content.append(Utils.gen_multiline_str(section_generator()))
         if self.has_gpu_recommendation():
             csp_report = csp_report_provider()
             if csp_report:
                 report_content.extend(csp_report)
         # append an empty line at the end of the report
         report_content.append('')
         return report_content
@@ -204,29 +205,66 @@
         # get the name of the cpu_cluster
         cpu_cluster_arg = offline_cluster_opts.get('cpuCluster')
         if cpu_cluster_arg is not None:
             cpu_cluster_obj = self._create_migration_cluster('CPU', cpu_cluster_arg)
             self.ctxt.set_ctxt('cpuClusterProxy', cpu_cluster_obj)
 
     def _process_gpu_cluster_args(self, offline_cluster_opts: dict = None) -> bool:
+        def _process_gpu_cluster_worker_node():
+            try:
+                worker_node = gpu_cluster_obj.get_worker_node()
+                worker_node._pull_and_set_mc_props(cli=self.ctxt.platform.cli)  # pylint: disable=protected-access
+                sys_info = worker_node._pull_sys_info(cli=self.ctxt.platform.cli)  # pylint: disable=protected-access
+                gpu_info = worker_node._pull_gpu_hw_info(cli=self.ctxt.platform.cli)  # pylint: disable=protected-access
+                worker_node.hw_info = NodeHWInfo(sys_info=sys_info, gpu_info=gpu_info)
+            except Exception:  # pylint: disable=broad-except
+                return
+
         gpu_cluster_arg = offline_cluster_opts.get('gpuCluster')
-        if gpu_cluster_arg is None:
-            self.logger.info('Creating GPU cluster by converting the CPU cluster instances to GPU supported types')
-            # Convert the CPU instances to support gpu
-            orig_cluster = self.ctxt.get_ctxt('cpuClusterProxy')
-            gpu_cluster_obj = self.ctxt.platform.migrate_cluster_to_gpu(orig_cluster)
-        else:
+        if gpu_cluster_arg:
             gpu_cluster_obj = self._create_migration_cluster('GPU', gpu_cluster_arg)
+        else:
+            orig_cluster = self.ctxt.get_ctxt('cpuClusterProxy')
+            gpu_cluster_obj = None
+            if orig_cluster:
+                # Convert the CPU instances to support gpu. Otherwise, gpuCluster is not set
+                self.logger.info('Creating GPU cluster by converting the CPU cluster instances to GPU supported types')
+                gpu_cluster_obj = self.ctxt.platform.migrate_cluster_to_gpu(orig_cluster)
         self.ctxt.set_ctxt('gpuClusterProxy', gpu_cluster_obj)
-        return True
+
+        _process_gpu_cluster_worker_node()
+        worker_node_hw_info = gpu_cluster_obj.get_worker_hw_info()
+        if gpu_cluster_obj:
+            self.ctxt.set_ctxt('recommendedConfigs', self._calculate_spark_settings(worker_node_hw_info))
+
+        return gpu_cluster_obj is not None
 
     def _process_offline_cluster_args(self):
         offline_cluster_opts = self.wrapper_options.get('migrationClustersProps', {})
         self._process_cpu_cluster_args(offline_cluster_opts)
-        self._process_gpu_cluster_args(offline_cluster_opts)
+        if self.ctxt.get_ctxt('cpuClusterProxy') is None:
+            # if no cpu-cluster is defined, then we are not supposed to run cost calculations
+            enable_savings_flag = False
+        else:
+            # if no gpu-cluster is defined, then we are not supposed to run cost calculations
+            enable_savings_flag = self._process_gpu_cluster_args(offline_cluster_opts)
+        self._set_savings_calculations_flag(enable_savings_flag)
+
+    def _set_savings_calculations_flag(self, enable_flag: bool):
+        self.ctxt.set_ctxt('enableSavingsCalculations', enable_flag)
+        if not enable_flag:
+            self.logger.info('Savings estimates are disabled because the cluster-information is '
+                             'not provided.')
+            # revisit the filtering-apps flag
+            if self.ctxt.get_ctxt('filterApps') == QualFilterApp.SAVINGS:
+                # When no cost calculations, the filters should be revisited
+                # set it to none
+                self.logger.info('Filtering criteria `filter_apps` will be reset to NONE because savings '
+                                 'estimates are disabled')
+                self.ctxt.set_ctxt('filterApps', QualFilterApp.NONE)
 
     def __process_gpu_cluster_recommendation(self, arg_val: str):
         available_types = [filter_enum.value for filter_enum in QualGpuClusterReshapeType]
         default_recommendation_txt = self.ctxt.get_value('sparkRapids', 'cli', 'defaults',
                                                          'gpuClusterRecommendation',
                                                          'defaultRecommendation')
         if arg_val:
@@ -285,28 +323,31 @@
         gpu_per_machine_arg = self.wrapper_options.get('gpuPerMachine')
         if gpu_per_machine_arg is not None:
             gpu_per_machine = gpu_per_machine_arg
         cuda = self.ctxt.get_value('sparkRapids', 'gpu', 'cudaVersion')
         cuda_arg = self.wrapper_options.get('cuda')
         if cuda_arg is not None:
             cuda = cuda_arg
-        target_platform = self.wrapper_options.get('target_platform')
+        target_platform = self.wrapper_options.get('targetPlatform')
         self.ctxt.set_ctxt('targetPlatform', target_platform)
         self.ctxt.set_ctxt('gpuPerMachine', gpu_per_machine)
         self.ctxt.set_ctxt('gpuDevice', gpu_device)
         self.ctxt.set_ctxt('cuda', cuda)
         # we need to process each argument to verify it is valid. otherwise, we may crash late
         self.__process_gpu_cluster_recommendation(self.wrapper_options.get('gpuClusterRecommendation'))
         self.__process_filter_args(self.wrapper_options.get('filterApps'))
 
         self._process_offline_cluster_args()
         self._process_eventlogs_args()
         # This is noise to dump everything
         # self.logger.debug('%s custom arguments = %s', self.pretty_name(), self.ctxt.props['wrapperCtx'])
 
+    def __is_savings_calc_enabled(self):
+        return self.ctxt.get_ctxt('enableSavingsCalculations')
+
     def __get_recommended_apps(self, all_rows, selected_cols=None) -> pd.DataFrame:
         speed_up_col = self.ctxt.get_value('toolOutput', 'csv', 'summaryReport',
                                            'recommendations', 'speedUp', 'columnName')
         recommended_vals = self.ctxt.get_value('toolOutput', 'csv', 'summaryReport',
                                                'recommendations', 'speedUp', 'selectedRecommendations')
         mask = all_rows[speed_up_col].isin(recommended_vals)
         if selected_cols is None:
@@ -385,14 +426,34 @@
                 conversion_items = []
                 for mc_src, mc_target in node_conversions.items():
                     conversion_items.append([mc_src, 'to', mc_target])
                 report_content.append(tabulate(conversion_items))
                 report_content.append(self.ctxt.platform.get_footer_message())
         return report_content
 
+    def __generate_recommended_configs_report(self) -> list:
+        report_content = []
+        if self.ctxt.get_ctxt('recommendedConfigs'):
+            conversion_items = []
+            recommended_configs = self.ctxt.get_ctxt('recommendedConfigs')
+            for config in recommended_configs:
+                conversion_items.append([config, recommended_configs[config]])
+            report_content.append(tabulate(conversion_items))
+        # the report should be appended to the log_summary file
+        rapids_output_dir = self.ctxt.get_rapids_output_folder()
+        rapids_log_file = FSUtil.build_path(rapids_output_dir,
+                                            self.ctxt.get_value('toolOutput', 'textFormat', 'summaryLog',
+                                                                'fileName'))
+        with open(rapids_log_file, 'a', encoding='UTF-8') as summary_log_file:
+            log_report = [Utils.gen_report_sec_header('Recommended Spark configurations for running on GPUs',
+                                                      hrule=False)]
+            log_report.extend(report_content)
+            summary_log_file.write(Utils.gen_multiline_str(log_report))
+        return report_content
+
     def __generate_cluster_shape_report(self) -> str:
         if bool(self.ctxt.platform.ctxt['notes']):
             return Utils.gen_multiline_str(self.ctxt.platform.ctxt['notes'].get('clusterShape'))
         return None
 
     def __recommendation_is_non_standard(self):
         cluster_shape_type = self.ctxt.get_ctxt('gpuClusterShapeRecommendation')
@@ -547,71 +608,77 @@
         if reshaped_notes:
             report_comments.append(reshaped_notes)
 
         pricing_config = self.ctxt.platform.configs.get_value_silent('pricing')
         target_platform = self.ctxt.get_ctxt('targetPlatform')
         if target_platform is not None:
             pricing_config = self.ctxt.platform.configs.get_value_silent('csp_pricing')
+        if pricing_config is None:
+            # OnPrem platform doesn't have pricing information. We do not calculate cost savings for
+            # OnPrem platform if the target_platform is not specified.
+            self.logger.warning('The pricing configuration for the given platform is not defined.\n\t'
+                                'Savings estimates cannot be generated.')
+        # enable savings report only if the price_config exists and the estimates are enabled
+        launch_savings_calc = self.__is_savings_calc_enabled() and (pricing_config is not None)
         reshape_col = self.ctxt.get_value('local', 'output', 'processDFProps',
                                           'clusterShapeCols', 'columnName')
         speed_recommendation_col = self.ctxt.get_value('local', 'output', 'speedupRecommendColumn')
         apps_reshaped_df, per_row_flag = self.__apply_gpu_cluster_reshape(apps_pruned_df)
-        # OnPrem platform doesn't have pricing information. We do not calculate cost savings for
-        # OnPrem platform if the target_platform is not specified.
-        if pricing_config is None:
-            df_final_result = apps_reshaped_df
-            if not apps_reshaped_df.empty:
-                # Do not include estimated job frequency in csv file
-                apps_reshaped_df = apps_reshaped_df.drop(columns=['Estimated Job Frequency (monthly)'])
-                self.logger.info('Generating GPU Estimated Speedup as %s', csv_out)
-                apps_reshaped_df.to_csv(csv_out, float_format='%.2f')
-        else:
+
+        if launch_savings_calc:
             # Now, the dataframe is ready to calculate the cost and the savings
             apps_working_set = self.__calc_apps_cost(apps_reshaped_df,
                                                      reshape_col,
                                                      speed_recommendation_col,
                                                      per_row_flag)
             df_final_result = apps_working_set
             if not apps_working_set.empty:
-                self.logger.info('Generating GPU Estimated Speedup and Savings as %s', csv_out)
+                self.logger.info('Generating GPU Estimated Speedup and Savings as: %s', csv_out)
                 # we can use the general format as well but this will transform numbers to E+. So, stick with %f
                 apps_working_set.to_csv(csv_out, float_format='%.2f')
+        else:
+            df_final_result = apps_reshaped_df
+            if not apps_reshaped_df.empty:
+                # Do not include estimated job frequency in csv file
+                apps_reshaped_df = apps_reshaped_df.drop(columns=['Estimated Job Frequency (monthly)'])
+                self.logger.info('Generating GPU Estimated Speedup: as %s', csv_out)
+                apps_reshaped_df.to_csv(csv_out, float_format='%.2f')
 
         return QualificationSummary(comments=report_comments,
                                     all_apps=apps_pruned_df,
                                     recommended_apps=recommended_apps,
-                                    pricing_config=pricing_config,
+                                    savings_report_flag=launch_savings_calc,
                                     df_result=df_final_result,
                                     irrelevant_speedups=speedups_irrelevant_flag,
                                     sections_generators=[self.__generate_mc_types_conversion_report])
 
     def _process_output(self):
         def process_df_for_stdout(raw_df):
             """
             process the dataframe to be more readable on the stdout
             1- convert time durations to second
             2- shorten headers
             """
-            selected_cols = self.ctxt.get_value('local', 'output', 'summaryColumns')
+            savings_report_enabled = self.__is_savings_calc_enabled()
+            # summary columns depend on the type of the generated report
+            selected_cols = self.ctxt.get_value('local', 'output', 'summaryColumns',
+                                                f'savingsReportEnabled{str(savings_report_enabled)}')
             # check if any filters apply
             filter_recommendation_enabled = self.ctxt.get_ctxt('filterApps') == QualFilterApp.SPEEDUPS
             filter_pos_enabled = self.ctxt.get_ctxt('filterApps') == QualFilterApp.SAVINGS
+
             if self.__recommendation_is_non_standard():
                 # During processing of arguments phase, we verified that the filter does not conflict
                 # with the shape recommendation
                 raw_df = self.__remap_cols_for_shape_type(raw_df,
                                                           selected_cols,
                                                           self.ctxt.get_ctxt('gpuClusterShapeRecommendation'))
                 # update the selected columns
                 selected_cols = list(raw_df.columns)
 
-            pricing_config = self.ctxt.platform.configs.get_value_silent('pricing')
-            target_platform = self.ctxt.get_ctxt('targetPlatform')
-            if pricing_config is None and target_platform is None:
-                selected_cols = list(raw_df.columns)
             # filter by recommendations if enabled
             if filter_recommendation_enabled:
                 df_row = self.__get_recommended_apps(raw_df, selected_cols)
             else:
                 df_row = raw_df.loc[:, selected_cols]
             if df_row.empty:
                 return df_row
@@ -671,25 +738,25 @@
             print(Utils.gen_multiline_str(wrapper_out_content))
 
     def _init_rapids_arg_list(self) -> List[str]:
         # TODO: Make sure we add this argument only for jar versions 23.02+
         return ['--platform', self.ctxt.platform.get_platform_name().replace('_', '-')]
 
     def _generate_section_lines(self, sec_conf: dict) -> List[str]:
+        # TODO: we may like to show the scripts even when the gpu-cluster is not defined
+        #      this requires that we allow to generate the script without the gpu-cluster
         if sec_conf.get('sectionID') == 'initializationScript':
             # format the initialization scripts
             reshaped_gpu_cluster = ClusterReshape(self.ctxt.get_ctxt('gpuClusterProxy'))
             gpu_per_machine, gpu_device = reshaped_gpu_cluster.get_gpu_per_worker()
             fill_map = {
                 0: self.ctxt.platform.cli.get_region(),
                 1: [gpu_device.lower(), gpu_per_machine]
             }
             res = []
-            # TODO: improve the display of code snippets by using module pygments (for bash)
-            #   module code can be used for python snippets
             for ind, l_str in enumerate(sec_conf['content'].get('lines')):
                 if ind in fill_map:
                     rep_var = fill_map.get(ind)
                     new_value = l_str.format(*rep_var) if isinstance(rep_var, list) else l_str.format(rep_var)
                     res.append(new_value)
                 else:
                     res.append(l_str)
@@ -701,14 +768,16 @@
             return ['```bash', highlighted_code, '```']
         if sec_conf.get('sectionID') == 'runUserToolsBootstrap':
             gpu_cluster = self.ctxt.get_ctxt('gpuClusterProxy')
             override_args = {'CLUSTER_NAME': '$CLUSTER_NAME'}
             script_content = gpu_cluster.generate_bootstrap_script(overridden_args=override_args)
             highlighted_code = TemplateGenerator.highlight_bash_code(script_content)
             return ['```bash', highlighted_code, '```', '']
+        if sec_conf.get('sectionID') == 'gpuBootstrapRecommendedConfigs':
+            return self.__generate_recommended_configs_report()
         return super()._generate_section_content(sec_conf)
 
 
 @dataclass
 class QualificationAsLocal(Qualification):
     """
     Qualification tool running on local development.
```

## spark_rapids_pytools/rapids/rapids_tool.py

```diff
@@ -23,15 +23,16 @@
 import time
 from concurrent.futures import ThreadPoolExecutor
 from dataclasses import dataclass, field
 from logging import Logger
 from typing import Any, Callable, Dict, List
 
 from spark_rapids_pytools.cloud_api.sp_types import CloudPlatform, get_platform, \
-    ClusterBase, DeployMode
+    ClusterBase, DeployMode, NodeHWInfo
+from spark_rapids_pytools.common.prop_manager import YAMLPropertiesContainer
 from spark_rapids_pytools.common.sys_storage import FSUtil
 from spark_rapids_pytools.common.utilities import ToolLogging, Utils
 from spark_rapids_pytools.rapids.rapids_job import RapidsJobPropContainer
 from spark_rapids_pytools.rapids.tool_ctxt import ToolContext
 
 
 @dataclass
@@ -306,18 +307,67 @@
     def _generate_platform_report_sections(self) -> List[str]:
         section_arr = self.ctxt.platform.configs.get_value_silent('wrapperReporting',
                                                                   self.name,
                                                                   'sections')
         if section_arr:
             rep_lines = []
             for curr_sec in section_arr:
-                rep_lines.extend(self._generate_section_content(curr_sec))
+                required_flag = curr_sec.get('requiresBoolFlag')
+                # if section requires a condition that was not enabled the section is skipped
+                if not required_flag or self.ctxt.get_ctxt(required_flag):
+                    rep_lines.extend(self._generate_section_content(curr_sec))
             return rep_lines
         return None
 
+    def _calculate_spark_settings(self, worker_info: NodeHWInfo) -> dict:
+        """
+        Calculate the cluster properties that we need to append to the /etc/defaults of the spark
+        if necessary.
+        :param worker_info: the hardware info as extracted from the worker. Note that we assume
+                            that all the workers have the same configurations.
+        :return: dictionary containing 7 spark properties to be set by default on the cluster.
+        """
+        num_gpus = worker_info.gpu_info.num_gpus
+        gpu_mem = worker_info.gpu_info.gpu_mem
+        num_cpus = worker_info.sys_info.num_cpus
+        cpu_mem = worker_info.sys_info.cpu_mem
+
+        config_path = Utils.resource_path('cluster-configs.yaml')
+        constants = YAMLPropertiesContainer(prop_arg=config_path).get_value('clusterConfigs', 'constants')
+        executors_per_node = num_gpus
+        num_executor_cores = max(1, num_cpus // executors_per_node)
+        gpu_concurrent_tasks = min(constants.get('maxGpuConcurrent'), gpu_mem // constants.get('gpuMemPerTaskMB'))
+        # account for system overhead
+        usable_worker_mem = max(0, cpu_mem - constants.get('systemReserveMB'))
+        executor_container_mem = usable_worker_mem // executors_per_node
+        # reserve 10% of heap as memory overhead
+        max_executor_heap = max(0, int(executor_container_mem * (1 - constants.get('heapOverheadFraction'))))
+        # give up to 2GB of heap to each executor core
+        executor_heap = min(max_executor_heap, constants.get('heapPerCoreMB') * num_executor_cores)
+        executor_mem_overhead = int(executor_heap * constants.get('heapOverheadFraction'))
+        # use default for pageable_pool to add to memory overhead
+        pageable_pool = constants.get('defaultPageablePoolMB')
+        # pinned memory uses any unused space up to 4GB
+        pinned_mem = min(constants.get('maxPinnedMemoryMB'),
+                         executor_container_mem - executor_heap - executor_mem_overhead - pageable_pool)
+        executor_mem_overhead += pinned_mem + pageable_pool
+        res = {
+            'spark.executor.cores': num_executor_cores,
+            'spark.executor.memory': f'{executor_heap}m',
+            'spark.executor.memoryOverhead': f'{executor_mem_overhead}m',
+            'spark.rapids.sql.concurrentGpuTasks': gpu_concurrent_tasks,
+            'spark.rapids.memory.pinnedPool.size': f'{pinned_mem}m',
+            'spark.sql.files.maxPartitionBytes': f'{constants.get("maxSqlFilesPartitionsMB")}m',
+            'spark.task.resource.gpu.amount': 1 / num_executor_cores,
+            'spark.rapids.shuffle.multiThreaded.reader.threads': num_executor_cores,
+            'spark.rapids.shuffle.multiThreaded.writer.threads': num_executor_cores,
+            'spark.rapids.sql.multiThreadedRead.numThreads': max(20, num_executor_cores)
+        }
+        return res
+
 
 @dataclass
 class RapidsJarTool(RapidsTool):
     """
     A wrapper class to represent wrapper commands that require RAPIDS jar file.
     """
```

## spark_rapids_pytools/resources/bootstrap-conf.yaml

```diff
@@ -4,29 +4,7 @@
   outputDir: bootstrap_tool_output
   cleanUp: true
 local:
   output:
     cleanUp: true
     # Name of the file where the final result is going to show
     fileName: rapids_4_dataproc_bootstrap_output.log
-  clusterConfigs:
-    constants:
-      # Maximum amount of pinned memory to use per executor in megabytes
-      maxPinnedMemoryMB: 4096
-      # Default pageable pool size per executor in megabytes
-      defaultPageablePoolMB: 1024
-      # Maximum number of concurrent tasks to run on the GPU
-      maxGpuConcurrent: 4
-      # Amount of GPU memory to use per concurrent task in megabytes
-      # Using a bit less than 8GB here since Dataproc clusters advertise
-      # T4s as only having around 14.75 GB and we want to run with
-      # 2 concurrent by default on T4s.
-      gpuMemPerTaskMB: 7500
-      # Ideal amount of JVM heap memory to request per CPU core in megabytes
-      heapPerCoreMB: 2048
-      # Fraction of the executor JVM heap size that should be additionally reserved
-      # for JVM off-heap overhead (thread stacks, native libraries, etc.)
-      heapOverheadFraction: 0.1
-      # Amount of CPU memory to reserve for system overhead (kernel, buffers, etc.) in megabytes
-      systemReserveMB: 2048
-      # By default set the spark.sql.files.maxPartitionBytes to 512m
-      maxSqlFilesPartitionsMB: 512
```

## spark_rapids_pytools/resources/dataproc-configs.json

### Pretty-printed

 * *Similarity: 0.9889740251068376%*

 * *Differences: {"'wrapperReporting'": "{'qualification': {'sections': {0: {'requiresBoolFlag': "*

 * *                       "'enableSavingsCalculations'}, 1: {'requiresBoolFlag': "*

 * *                       "'enableSavingsCalculations'}, 3: {'content': {'header': {insert: [(1, 'To "*

 * *                       "generate the recommended configurations on an existing GPU-Cluster,'), (2, "*

 * *                       "'  re-run the Bootstrap tool to provide optimized RAPIDS Accelerator'), "*

 * *                       "(3, '  for Apache Spark con []*

```diff
@@ -250,40 +250,58 @@
                             "  following to your cluster creation script:"
                         ],
                         "lines": [
                             "    --initialization-actions=gs://goog-dataproc-initialization-actions-{}/spark-rapids/spark-rapids.sh \\",
                             "    --worker-accelerator type=nvidia-tesla-{},count={}"
                         ]
                     },
+                    "requiresBoolFlag": "enableSavingsCalculations",
                     "sectionID": "initializationScript",
                     "sectionName": "Initialization Scripts"
                 },
                 {
                     "content": {
                         "header": [
                             "",
                             "To create a GPU cluster, run the following script:",
                             ""
                         ]
                     },
+                    "requiresBoolFlag": "enableSavingsCalculations",
                     "sectionID": "gpuClusterCreationScript"
                 },
                 {
                     "content": {
                         "header": [
                             "",
-                            "Once the cluster is created, run the Bootstrap tool to provide optimized",
-                            "RAPIDS Accelerator for Apache Spark configs based on GPU cluster shape.",
-                            "Notes:",
+                            "For the new GPU-accelerated cluster with RAPIDS Accelerator for Apache Spark,",
+                            "  it is recommended to set the following Spark configurations:",
+                            ""
+                        ]
+                    },
+                    "requiresBoolFlag": "enableSavingsCalculations",
+                    "sectionID": "gpuBootstrapRecommendedConfigs",
+                    "sectionName": "Recommended Spark configurations for running on GPUs"
+                },
+                {
+                    "content": {
+                        "header": [
+                            "",
+                            "To generate the recommended configurations on an existing GPU-Cluster,",
+                            "  re-run the Bootstrap tool to provide optimized RAPIDS Accelerator",
+                            "  for Apache Spark configs based on GPU cluster shape.",
+                            "  Notes:",
                             "    - Overriding the Apache Spark default configurations on the cluster",
                             "      requires SSH access.",
                             "    - If SSH access is unavailable, you can still dump the recommended",
                             "      configurations by enabling the `dry_run` flag.",
                             ""
                         ]
                     },
-                    "sectionID": "runUserToolsBootstrap"
+                    "requiresBoolFlag": "DISABLED",
+                    "sectionID": "runUserToolsBootstrap",
+                    "sectionName": "Regenerating recommended configurations for an existing GPU-Cluster"
                 }
             ]
         }
     }
 }
```

## spark_rapids_pytools/resources/emr-configs.json

### Pretty-printed

 * *Similarity: 0.9876135149572649%*

 * *Differences: {"'wrapperReporting'": "{'qualification': {'sections': {0: {'requiresBoolFlag': "*

 * *                       "'enableSavingsCalculations'}, 2: {'content': {'header': {insert: [(1, 'To "*

 * *                       "generate the recommended configurations on an existing GPU-Cluster,'), (2, "*

 * *                       "'  re-run the Bootstrap tool to provide optimized RAPIDS Accelerator'), "*

 * *                       "(3, '  for Apache Spark configs based on GPU cluster shape.'), (4, '  "*

 * *                       "Notes:')],  []*

```diff
@@ -269,30 +269,47 @@
                     "content": {
                         "header": [
                             "",
                             "To create a GPU cluster, run the following script:",
                             ""
                         ]
                     },
+                    "requiresBoolFlag": "enableSavingsCalculations",
                     "sectionID": "gpuClusterCreationScript",
                     "sectionName": "Initialization Scripts"
                 },
                 {
                     "content": {
                         "header": [
                             "",
-                            "Once the cluster is created, run the Bootstrap tool to provide optimized",
-                            "RAPIDS Accelerator for Apache Spark configs based on GPU cluster shape.",
-                            "Notes:",
+                            "For the new GPU-accelerated cluster with RAPIDS Accelerator for Apache Spark,",
+                            "  it is recommended to set the following Spark configurations:",
+                            ""
+                        ]
+                    },
+                    "requiresBoolFlag": "enableSavingsCalculations",
+                    "sectionID": "gpuBootstrapRecommendedConfigs",
+                    "sectionName": "Recommended Spark configurations for running on GPUs"
+                },
+                {
+                    "content": {
+                        "header": [
+                            "",
+                            "To generate the recommended configurations on an existing GPU-Cluster,",
+                            "  re-run the Bootstrap tool to provide optimized RAPIDS Accelerator",
+                            "  for Apache Spark configs based on GPU cluster shape.",
+                            "  Notes:",
                             "    - Overriding the Apache Spark default configurations on the cluster",
                             "      requires SSH access.",
                             "    - If SSH access is unavailable, you can still dump the recommended",
                             "      configurations by enabling the `dry_run` flag.",
                             ""
                         ]
                     },
-                    "sectionID": "runUserToolsBootstrap"
+                    "requiresBoolFlag": "DISABLED",
+                    "sectionID": "runUserToolsBootstrap",
+                    "sectionName": "Regenerating recommended configurations for an existing GPU-Cluster"
                 }
             ]
         }
     }
 }
```

## spark_rapids_pytools/resources/qualification-conf.yaml

```diff
@@ -1,10 +1,13 @@
 toolOutput:
   completeOutput: true
   subFolder: rapids_4_spark_qualification_output
+  textFormat:
+    summaryLog:
+      fileName: rapids_4_spark_qualification_output.log
   csv:
     summaryReport:
       fileName: rapids_4_spark_qualification_output.csv
       columns:
         - App Name
         - App ID
         - Recommendation
@@ -89,23 +92,31 @@
       - 'Estimated GPU Cost'
       - 'Estimated GPU Savings(%)'
       - 'Estimated Job Frequency (monthly)'
       - 'Annual Cost Savings'
     savingColumn: 'Estimated GPU Savings(%)'
     speedupRecommendColumn: 'Speedup Based Recommendation'
     savingRecommendColumn: 'Savings Based Recommendation'
-    summaryColumns:
-      - 'App ID'
-      - 'App Name'
-      - 'Speedup Based Recommendation'
-      - 'Savings Based Recommendation'
-      - 'App Duration'
-      - 'Estimated GPU Duration'
-      - 'Estimated GPU Speedup'
-      - 'Estimated GPU Savings(%)'
+    summaryColumns: #columns to be displayed on the stdout as part of the final report
+      savingsReportEnabledTrue: #columns with savings estimates enabled
+        - 'App ID'
+        - 'App Name'
+        - 'Speedup Based Recommendation'
+        - 'Savings Based Recommendation'
+        - 'App Duration'
+        - 'Estimated GPU Duration'
+        - 'Estimated GPU Speedup'
+        - 'Estimated GPU Savings(%)'
+      savingsReportEnabledFalse: #columns with savings estimates disabled
+        - 'App ID'
+        - 'App Name'
+        - 'Speedup Based Recommendation'
+        - 'App Duration'
+        - 'Estimated GPU Duration'
+        - 'Estimated GPU Speedup'
     processDFProps:
       minimumWorkerCount: 2
       gpuScaleFactor: 0.80
       savingRecommendationsRanges:
         nonRecommended:
           title: 'Not Recommended'
           lowerBound: -1000000.0
@@ -148,8 +159,7 @@
           - '^(\.+).*'
           - '^(\$+).*'
           - '^.+(_\$folder\$)$'
 platform:
   shortName: 'qual'
   outputDir: qual-tool-output
   cleanUp: true
-
```

## spark_rapids_pytools/wrappers/databricks_aws_wrapper.py

```diff
@@ -23,15 +23,15 @@
 
 class CliDBAWSLocalMode:  # pylint: disable=too-few-public-methods
     """
     A wrapper that runs the RAPIDS Accelerator tools locally on the dev machine for DATABRICKS_AWS.
     """
 
     @staticmethod
-    def qualification(cpu_cluster: str,
+    def qualification(cpu_cluster: str = None,
                       eventlogs: str = None,
                       profile: str = None,
                       aws_profile: str = None,
                       local_folder: str = None,
                       remote_folder: str = None,
                       gpu_cluster: str = None,
                       tools_jar: str = None,
```

## spark_rapids_pytools/wrappers/databricks_azure_wrapper.py

```diff
@@ -23,15 +23,15 @@
 
 class CliDBAzureLocalMode:  # pylint: disable=too-few-public-methods
     """
     A wrapper that runs the RAPIDS Accelerator tools locally on the dev machine for DATABRICKS_AZURE.
     """
 
     @staticmethod
-    def qualification(cpu_cluster: str,
+    def qualification(cpu_cluster: str = None,
                       eventlogs: str = None,
                       profile: str = None,
                       local_folder: str = None,
                       remote_folder: str = None,
                       gpu_cluster: str = None,
                       tools_jar: str = None,
                       credentials_file: str = None,
```

## spark_rapids_pytools/wrappers/dataproc_wrapper.py

```diff
@@ -25,15 +25,15 @@
 
 class CliDataprocLocalMode:  # pylint: disable=too-few-public-methods
     """
     A wrapper that runs the RAPIDS Accelerator tools locally on the dev machine for Dataproc.
     """
 
     @staticmethod
-    def qualification(cpu_cluster: str,
+    def qualification(cpu_cluster: str = None,
                       eventlogs: str = None,
                       local_folder: str = None,
                       remote_folder: str = None,
                       gpu_cluster: str = None,
                       tools_jar: str = None,
                       credentials_file: str = None,
                       filter_apps: str = QualFilterApp.tostring(QualFilterApp.SAVINGS),
```

## spark_rapids_pytools/wrappers/emr_wrapper.py

```diff
@@ -25,15 +25,15 @@
 
 class CliEmrLocalMode:  # pylint: disable=too-few-public-methods
     """
     A wrapper that runs the RAPIDS Accelerator tools locally on the dev machine.
     """
 
     @staticmethod
-    def qualification(cpu_cluster: str,
+    def qualification(cpu_cluster: str = None,
                       eventlogs: str = None,
                       profile: str = None,
                       local_folder: str = None,
                       remote_folder: str = None,
                       gpu_cluster: str = None,
                       tools_jar: str = None,
                       filter_apps: str = QualFilterApp.tostring(QualFilterApp.SAVINGS),
```

## spark_rapids_pytools/wrappers/onprem_wrapper.py

```diff
@@ -99,15 +99,15 @@
                     'jvmMaxHeapSize': jvm_heap_size
                 }
             },
             'eventlogs': eventlogs,
             'filterApps': filter_apps,
             'toolsJar': tools_jar,
             'gpuClusterRecommendation': gpu_cluster_recommendation,
-            'target_platform': target_platform
+            'targetPlatform': target_platform
         }
         tool_obj = QualificationAsLocal(platform_type=CloudPlatform.ONPREM,
                                         output_folder=local_folder,
                                         wrapper_options=wrapper_qual_options,
                                         rapids_options=rapids_options)
         tool_obj.launch()
```

## Comparing `spark_rapids_user_tools-23.6.2.dist-info/LICENSE` & `spark_rapids_user_tools-23.6.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `spark_rapids_user_tools-23.6.2.dist-info/METADATA` & `spark_rapids_user_tools-23.6.3.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spark-rapids-user-tools
-Version: 23.6.2
+Version: 23.6.3
 Summary: A simple wrapper process around cloud service providers to run tools for the RAPIDS Accelerator for Apache Spark.
 Author-email: NVIDIA Corporation <spark-rapids-support@nvidia.com>
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
@@ -15,15 +15,15 @@
 Requires-Dist: fire (>=0.5.0)
 Requires-Dist: pandas (==1.4.3)
 Requires-Dist: pyYAML (==6.0)
 Requires-Dist: tabulate (==0.8.10)
 Requires-Dist: importlib-resources (==5.10.2)
 Requires-Dist: requests (==2.31.0)
 Requires-Dist: packaging (==23.0)
-Requires-Dist: certifi (==2022.12.7)
+Requires-Dist: certifi (==2023.7.22)
 Requires-Dist: idna (==3.4)
 Requires-Dist: urllib3 (==1.26.14)
 Requires-Dist: beautifulsoup4 (==4.11.2)
 Requires-Dist: pygments (==2.15.0)
 
 # spark-rapids-user-tools
```

## Comparing `spark_rapids_user_tools-23.6.2.dist-info/RECORD` & `spark_rapids_user_tools-23.6.3.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,68 +1,69 @@
-spark_rapids_pytools/__init__.py,sha256=v4w4dPmBMO2Koe_vJ0T3X8-SDijhkEnnLRiPmH3eJKI,750
+spark_rapids_pytools/__init__.py,sha256=GNY01s_akMvu3rN6WmfVymGq6LuEdpxZsDAXrSLFxE4,750
 spark_rapids_pytools/build.py,sha256=Ej4Pc2jPIyeVUUOyC67ZMc6Tj90NKj0DrXyniJc0FnY,992
 spark_rapids_pytools/wrapper.py,sha256=mKTxIpK3V_OKJ4ua3TSjasC5mITs_TRHU5dRfCqcde0,1324
 spark_rapids_pytools/ascli_cli/__init__.py,sha256=i0t3LTjoAuxT5vp9ikODfXK3OnvCNYjFymqk5KMDMf4,627
-spark_rapids_pytools/ascli_cli/ascli.py,sha256=Y397MBIGTTPWiPLRrEb-5XJLNrtd17cYZDcChF7W5xE,8270
+spark_rapids_pytools/ascli_cli/ascli.py,sha256=QufvJYqWuxLQ1RJdySfA-Fgp8bygF6snC4oEHsDSrF4,8548
 spark_rapids_pytools/cloud_api/__init__.py,sha256=NQSbmxhLzvnZrf4ltpgCLMjCEERPv5QoYo62LEdDBs8,646
 spark_rapids_pytools/cloud_api/azurestorage.py,sha256=yuFk7H5Y8aY0H5dOmeJBS03uhgS2nah5v0Kw2GJzDnE,8507
 spark_rapids_pytools/cloud_api/databricks_aws.py,sha256=W0qOOwfTzzW2MK-fL5V7mXvpz17-DbmllIlCD-cViG4,12662
 spark_rapids_pytools/cloud_api/databricks_aws_job.py,sha256=ZNYM2pa8fObRhc9c9VcqCT6HxGJNfHeFhdoVYNek51E,1268
 spark_rapids_pytools/cloud_api/databricks_azure.py,sha256=QcOoVpkp9waGsgELqkDW1dj73PruZN-vD2d4EK7u--Y,16308
 spark_rapids_pytools/cloud_api/databricks_azure_job.py,sha256=u-wOcJXRyFCuEsXQypW8O09O5YQ2pt_gNcOlQjwpXYE,2428
 spark_rapids_pytools/cloud_api/dataproc.py,sha256=iQZvIyu__ge2Wf_fRKpjBVKWfOkI4DENxO8ZLy1lsqs,24674
 spark_rapids_pytools/cloud_api/dataproc_job.py,sha256=VMBbFjUfzF7ZZtwWI5dsc8exiZwkARHQIeWIcY3wZNs,922
 spark_rapids_pytools/cloud_api/emr.py,sha256=qZtOUIR7UpSDAOn292lwlMO5sx5-UjEdSTHS8sT5LPk,21033
 spark_rapids_pytools/cloud_api/emr_job.py,sha256=-6VA4HV34BVm_CoxaSh13pDb1pyeYHoRbzWVQFfuT0o,1252
 spark_rapids_pytools/cloud_api/gstorage.py,sha256=88IzLWOzEphCETGQaVN3_U7wUD0M2doDy759WQmNoJI,4939
-spark_rapids_pytools/cloud_api/onprem.py,sha256=hrsUe0KUa6qEMYiXgse5myDtllE25K1C3AikEJvXrkU,13387
+spark_rapids_pytools/cloud_api/onprem.py,sha256=5xtyLtDKykBZooTbJ3q3itm-CAcumSyxUwlsWoLQGko,13310
 spark_rapids_pytools/cloud_api/s3storage.py,sha256=cFqZETxWt_3Yagk3UlQt65pM4A0qb6idpbi8kUOK5pE,4055
-spark_rapids_pytools/cloud_api/sp_types.py,sha256=3FSnFir4jP9VvHcM1bWd3DifT4XduhYp47BlhJEZQLM,51024
+spark_rapids_pytools/cloud_api/sp_types.py,sha256=xlNazokYEaDLrT9B1-rizrZzTt-0ifDLDyM45olUjsI,51086
 spark_rapids_pytools/common/__init__.py,sha256=A8h0t211p8t_aATYiwCLWTwTy564kCcZp-HTWKiO4u8,658
 spark_rapids_pytools/common/exceptions.py,sha256=CK9PF75hSrRh6qaz0aKoiNqaU78E8OfiNydZ4i6WgoM,978
 spark_rapids_pytools/common/prop_manager.py,sha256=ijCqsnrLHn8Ntb1dCEDHqITrxjb3LoEP8Eqhqsb0PS8,5432
 spark_rapids_pytools/common/sys_storage.py,sha256=-La2Oc5EemnRH2x5B24MHlrtEZh6nL2FjqAyrV953Os,14712
 spark_rapids_pytools/common/utilities.py,sha256=VtS0shH3GhOTGpHHZNYdH4p1ziKKY1h14Q_qfRi7XQk,14006
 spark_rapids_pytools/pricing/__init__.py,sha256=Y_IWTtiKulkkoC84SLS8LkRWTi393zeUGHaNogZfOSg,659
 spark_rapids_pytools/pricing/databricks_azure_pricing.py,sha256=HLto1Dbv61Awfo4icJiRiCkx3G8KDRfUBiTjloEy1WM,3429
 spark_rapids_pytools/pricing/databricks_pricing.py,sha256=q7pRBs2YGJAJnxItBUDYB00f3hdWsFKgK4sXZocjMC4,3522
 spark_rapids_pytools/pricing/dataproc_pricing.py,sha256=4-RuYxW9V0K0mqj2mqCJsKEUbvxPXHCwF4AsK1z6UXE,4247
 spark_rapids_pytools/pricing/emr_pricing.py,sha256=vat14Df8_1By-McNl2ot_75RAwjJkyTfXA24-iefrM4,4717
 spark_rapids_pytools/pricing/price_provider.py,sha256=zZt2ay-BN1vM0cZXkiEzY5kpnYx1brRCJcyfotU71lo,6400
 spark_rapids_pytools/rapids/__init__.py,sha256=xiYk9b76AV_v3fEELcYzXCUPvXQhCD687eJ5RCYQW7M,666
-spark_rapids_pytools/rapids/bootstrap.py,sha256=KEeF1Ux0mfqCW-6oyQOoImSuCm_MmoITm_t6qg9yB4s,8830
+spark_rapids_pytools/rapids/bootstrap.py,sha256=V2-inHdECJYpPXQMuO1KhWvYIGF8M2fYq7fcXVnwAWo,6026
 spark_rapids_pytools/rapids/diagnostic.py,sha256=pShr5Ndi8kxZXTbZ7fOkbWVk2opF1UYP0q4AJETGHGc,6546
 spark_rapids_pytools/rapids/profiling.py,sha256=ufyIUOxRXeZGPBRQ2jH3d0-qQ4vzUe2HNrKtmYkZzgs,14549
-spark_rapids_pytools/rapids/qualification.py,sha256=kiYkjgTrUPH5z6RenxJJt1w26khHfYy-nzUCIzh7vjQ,38670
+spark_rapids_pytools/rapids/qualification.py,sha256=mgKF1_8YhUUdL6HBMAOWili4GJTus4ODp-7wUgXqOUk,42806
 spark_rapids_pytools/rapids/rapids_job.py,sha256=HId6wvfXwsPVb26meKGn_C1PcS68NGT41uNgZZx6vTY,5739
-spark_rapids_pytools/rapids/rapids_tool.py,sha256=SX8GpcHRQB3v4hJgKD2dJNz8rjcZMyf2wwdcRKTGEHs,33598
+spark_rapids_pytools/rapids/rapids_tool.py,sha256=g4SA6ZZdBi1WmFoQ0yMyODXayF-goAnB9OIaeHr_MjI,36808
 spark_rapids_pytools/rapids/tool_ctxt.py,sha256=8bWqooTKcIH5ShxTFrXPuSr1KEJeHXU5LdDq8F6ienU,6576
-spark_rapids_pytools/resources/bootstrap-conf.yaml,sha256=xATXdInBA2NFR2Le9uqioRwyqYB7TnFBYvoaT8NEDZQ,1390
+spark_rapids_pytools/resources/bootstrap-conf.yaml,sha256=rqjuQ2gwyOmavD3c8q_mWM5qXCxThczg2ldyjdGwgw4,273
+spark_rapids_pytools/resources/cluster-configs.yaml,sha256=5gklmC6mR1EeKvpeL-m0zrFJGaQnDiBGeXRFGdCoXqM,1073
 spark_rapids_pytools/resources/collect.sh,sha256=EdbNF90ZQUgaERPYY1p3fYdf3qvmU9f2WGD7RzYoLOE,4453
 spark_rapids_pytools/resources/databricks-premium-catalog.json,sha256=XBptMDeu7Abbrv6xC4C1oDuMuEIqnrGEqwrhJFutLJM,30566
 spark_rapids_pytools/resources/databricks_aws-configs.json,sha256=I_VqLdvavbQGUqCNcUqMpNrH5aF0xBZDdN7kuncNKQk,10448
 spark_rapids_pytools/resources/databricks_azure-configs.json,sha256=TSo17t28bRfeAjSWDZR4KqMhEUKoLzoqO5YknfiokYA,10017
-spark_rapids_pytools/resources/dataproc-configs.json,sha256=9rEXtuCKsLUMyoRsKAeW2OWckM1AXYsE3JuNjFAYmWA,8047
+spark_rapids_pytools/resources/dataproc-configs.json,sha256=7asYXexJQP3ihH6Z5LpWdW4t5gUW1AW6x3-GdcTqzDA,8872
 spark_rapids_pytools/resources/diagnostic-conf.yaml,sha256=vS32UyRhj5ox2MZ8-6EgMjXEWlGeaw9izYiAdAoWpfE,30
-spark_rapids_pytools/resources/emr-configs.json,sha256=_F5XxM5kFen_iCAWW5Z_jdD6G626h4hH_Uf2dq2Dn6U,8994
+spark_rapids_pytools/resources/emr-configs.json,sha256=Ac5PO1zSuFCNcZq4CM1csLK_htdcnUUNHX7-4F6jGEU,9760
 spark_rapids_pytools/resources/onprem-configs.json,sha256=TWqp95Ildys0-eGOrWNL-KpUqTTLVHLqzW4u48eNmhM,4727
 spark_rapids_pytools/resources/premium-databricks-azure-catalog.json,sha256=KGxUoiNPVTVzQaR3SN_FV9PH80fEnp6JnMhnv2YFOBY,38903
 spark_rapids_pytools/resources/profiling-conf.yaml,sha256=jdJy6I_alouAk_Ff39r0a3D8EAByaKK2r979bAjPTrQ,1524
-spark_rapids_pytools/resources/qualification-conf.yaml,sha256=FwQGa9cFXQbEutN9aGb9IEzgRFdHTC7fP_QYTrFWmyc,4273
+spark_rapids_pytools/resources/qualification-conf.yaml,sha256=ybMrK7B27FiBSTCRtxET0BDYxts0P9b1_I4ZuD3A9J8,4762
 spark_rapids_pytools/resources/dev/process_databricks_azure_pricing.py,sha256=lmXGoTdrwdrhh8UO7DSL-kgbLOJP1X0kgJPq6cJfF3U,2195
 spark_rapids_pytools/resources/templates/dataproc-create_gpu_cluster_script.ms,sha256=pigL0kAsg1VaP9Jn9-5oNblJRj0IbeZLh1T5Op8CQqA,671
 spark_rapids_pytools/resources/templates/dataproc-run_bootstrap.ms,sha256=lmnyouNCpbytpRVZ_YbQ6d6hCl3XaYxqzNomxSJwSSk,518
 spark_rapids_pytools/resources/templates/emr-create_gpu_cluster_script.ms,sha256=UIlW0GcYn2fovtcDNYPUmg99h0zKy-DxXGO-1Lr_xQs,855
 spark_rapids_pytools/resources/templates/emr-run_bootstrap.ms,sha256=7xqXBMIu5Tg02KCq_YN9oLLiE3c8jgWl8BUvFQZODhs,537
 spark_rapids_pytools/resources/templates/info_recommendations_disabled.ms,sha256=YwNa8iY9MhoDFWU_KfgPIO1hdfHssWXZw51dYXbM9T8,424
 spark_rapids_pytools/wrappers/__init__.py,sha256=CQ7Mf-YyBFNl6xmQGsPARv1w5GU3jGliByn5IHCcLkE,630
-spark_rapids_pytools/wrappers/databricks_aws_wrapper.py,sha256=-axS_C8sb3RvlOByfk6ZqYQvOSJ36GJwk4zwqiZ-t-c,13207
-spark_rapids_pytools/wrappers/databricks_azure_wrapper.py,sha256=fd77N19GhREtG8P4rTQu4w64BB0on37b9iT5p6PPmEo,12960
-spark_rapids_pytools/wrappers/dataproc_wrapper.py,sha256=EXpizTdx6gve5kYY5vxAONITRehxhu3E_oooqUffvEY,15910
-spark_rapids_pytools/wrappers/emr_wrapper.py,sha256=kbhlIdfnNbur6voSG6X0rDxzeFB9AB11TQYT482K6lo,12086
-spark_rapids_pytools/wrappers/onprem_wrapper.py,sha256=DOWHGDKToGHMiu9uDfgjnRgY7wWuVZsffvjeSwp-lXw,9829
-spark_rapids_user_tools-23.6.2.dist-info/LICENSE,sha256=RnI8IUCDrXfGVHFfYTsT8OKEuaOtkMGDQOn8foh7D00,21086
-spark_rapids_user_tools-23.6.2.dist-info/METADATA,sha256=-zFTDwYLAfOTOE7oWSqPTLxIf7AoMRa38CN9ZwiLzJg,2927
-spark_rapids_user_tools-23.6.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-spark_rapids_user_tools-23.6.2.dist-info/entry_points.txt,sha256=xiWYwYIymak3h07eV4KehH9gksbSubHnVFaKhfanJqk,128
-spark_rapids_user_tools-23.6.2.dist-info/top_level.txt,sha256=47ZJrA6QPYYeI7JxABL1bTLJ7H9haiGWrN-6jVLYOXU,21
-spark_rapids_user_tools-23.6.2.dist-info/RECORD,,
+spark_rapids_pytools/wrappers/databricks_aws_wrapper.py,sha256=aRRrc0nFZfnJNsNulZgzh5guoAYPIyjxIrFmA_hilW4,13214
+spark_rapids_pytools/wrappers/databricks_azure_wrapper.py,sha256=tkHjSfngyhkYOe1fJJYZVhBd0WCzgnyNDJM35Zr5GKc,12967
+spark_rapids_pytools/wrappers/dataproc_wrapper.py,sha256=0gcxCfekrj4_LzNo-r48mPIn-wMOvNrmVL0RXdjal80,15917
+spark_rapids_pytools/wrappers/emr_wrapper.py,sha256=1ymShP8qi7Y8DMzQNKng6gZG-mPEQGE4FPJTztWSjk0,12093
+spark_rapids_pytools/wrappers/onprem_wrapper.py,sha256=ofQJAa2RmRK54hEWlUiSzigdHzrhw2ewoH2mG0GhtMM,9828
+spark_rapids_user_tools-23.6.3.dist-info/LICENSE,sha256=RnI8IUCDrXfGVHFfYTsT8OKEuaOtkMGDQOn8foh7D00,21086
+spark_rapids_user_tools-23.6.3.dist-info/METADATA,sha256=B_FBDJ0J3Qq9A_JnQ6ht_sFf3ltEWVC0DUualnsme2U,2927
+spark_rapids_user_tools-23.6.3.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+spark_rapids_user_tools-23.6.3.dist-info/entry_points.txt,sha256=xiWYwYIymak3h07eV4KehH9gksbSubHnVFaKhfanJqk,128
+spark_rapids_user_tools-23.6.3.dist-info/top_level.txt,sha256=47ZJrA6QPYYeI7JxABL1bTLJ7H9haiGWrN-6jVLYOXU,21
+spark_rapids_user_tools-23.6.3.dist-info/RECORD,,
```

